# -*- coding: utf-8 -*-
"""Information Extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUCweGT5Wa6GR3u8NMEJDRqQQ-Sh3OFu
"""

!pip install spark-nlp
!pip install pyspark

import sparknlp

# Start Spark Session
spark = sparknlp.start()

# Import the required modules and classes
from sparknlp.base import DocumentAssembler, Pipeline
from sparknlp.annotator import (
    RegexMatcher
)
from pyspark.sql.types import StringType
import pyspark.sql.functions as F

# Step 1: Transform raw texts to `document` annotation
documentAssembler = DocumentAssembler()\
    .setInputCol("text")\
    .setOutputCol("document")

# Step 2: Define the regex rules
regex_matcher = RegexMatcher()\
    .setRules(["\d{4}\/\d\d\/\d\d,date", "\s\d{2}\/\d\d\/\d\d,short_date"]) \ ##setRules - Provides regex patterns and assigns labels
    .setDelimiter(",") \. ## Separates regex pattern and label using a comma
    .setInputCols(["document"]) \.     ## Takes document column (usually from a DocumentAssembler) as input
    .setOutputCol("matched_text") \.   ## Stores matched results in this column
    .setStrategy("MATCH_ALL").         ## Returns all matches, not just the first one
## "\d{4}\/\d\d\/\d\d,date" - means Matches a full date like 2024/06/25 and labels it date
## "\s\d{2}\/\d\d\/\d\d,short_date" - means -Matches short date like 25/06/24 (with leading space) and labels it short_date

# Define the Pipeline
nlpPipeline = Pipeline(stages=[documentAssembler,regex_matcher])

# Make a sample text
text_list = ["John Smith was born on 1965/12/10 in New York City.",
"He got married on 89/01/05 and started his new job on 91/03/11.",
"He went on a trip to Europe from 2022/06/11/2022 to 2022/07/05 and visited 6 countries.",
"He celebrated his 10th wedding anniversary on 1999/01/05."]

# Convert the sample_text to a dataframe
spark_df = spark.createDataFrame(text_list, StringType()).toDF("text")

# Fit and transform to get a prediction
result = nlpPipeline.fit(spark_df).transform(spark_df)

result.select(F.explode(F.arrays_zip(result.matched_text.result,
                                     result.matched_text.metadata)).alias("cols")) \
       .select(F.expr("cols['0']").alias("Matches Found"),
               F.expr("cols['1']['identifier']").alias("identifier")).show()